# AI-Code Review Checklist

Structured checklist for AI-specific failure modes in code generated by agent teams. Injected into the Quality Reviewer's task in review mode spawns. Each item includes a description, search pattern, and severity.

## Why This Exists

AI-generated code exhibits distinct failure patterns that differ from human-written code. Standard code review catches logic errors and style issues but misses patterns like over-abstraction (wrapper classes with no logic), phantom error handling (catch blocks that only log), and test theater (tests that verify mocks, not behavior). This checklist gives reviewers specific, actionable things to search for.

## Failure Modes

### 1. Over-Abstraction

**Description:** Wrapper classes, utility functions, or abstraction layers that add indirection without adding logic. The AI creates "clean architecture" scaffolding that passes calls through without transformation.

**Search pattern:** Look for classes or functions where every method delegates to a single dependency with no additional logic, transformation, or branching.

**Examples:**
- Service class that wraps every repository method 1:1
- Utility function that calls a library function with identical parameters
- Abstract base class with only one implementation

**Severity:** Medium

---

### 2. Phantom Error Handling

**Description:** Catch blocks that log the error and either rethrow unchanged or swallow it. The code appears to handle errors but provides no recovery, transformation, or meaningful degradation.

**Search pattern:** Look for catch blocks where the body contains only logging statements, or logging followed by `throw` of the same error. Also check for empty catch blocks.

**Examples:**
- `catch (error) { console.error(error); throw error; }`
- `catch (e) { logger.warn('Failed', e); }` with no fallback behavior
- `.catch(err => { /* TODO */ })`

**Severity:** High

---

### 3. Hallucinated Dependencies

**Description:** Imports of packages, modules, or APIs that don't exist in the project's dependency tree. The AI "remembers" packages from training data that aren't installed.

**Search pattern:** Cross-reference import statements against `package.json` dependencies (or equivalent manifest). Check for imports from paths that don't exist in the project.

**Examples:**
- `import { validateEmail } from 'email-validator'` when package isn't in dependencies
- `from utils.validators import ...` referencing a module that doesn't exist
- Using API methods that don't exist on the imported library's actual interface

**Severity:** Critical

---

### 4. Test Theater

**Description:** Tests that verify mock behavior rather than real behavior. The test passes because the mock is configured to return the expected value, not because the code works correctly.

**Search pattern:** Look for tests where the assertion verifies a value that was directly set up in the mock/stub. Also check for tests with no assertions, or tests that only verify a function was called (not what it produced).

**Examples:**
- Mock returns `{ id: 1, name: 'Test' }`, test asserts result equals `{ id: 1, name: 'Test' }`
- Test mocks the database, mocks the service, mocks the controller â€” tests nothing real
- `expect(mockFn).toHaveBeenCalled()` as the only assertion

**Severity:** High

---

### 5. Redundant Validation

**Description:** Identical validation logic duplicated across multiple layers without purpose. The AI adds "defensive" checks at every boundary, creating maintenance burden without safety benefit.

**Search pattern:** Look for the same validation check (null checks, type checks, format validation) repeated in the controller, service, and repository layers for the same data path.

**Examples:**
- `if (!userId) throw new Error('...')` in controller, service, AND repository
- Email format regex validated in form, API handler, and database model
- Null checks on the same parameter at 3+ levels of the call stack

**Severity:** Low

---

### 6. Missing Idiomatic Patterns

**Description:** Code that works but ignores established patterns in the codebase. The AI writes generic solutions instead of following project conventions for routing, error handling, data access, or component structure.

**Search pattern:** Compare the new code's patterns against 2-3 existing files in the same directory. Look for: different error handling strategy, different naming conventions, different file structure, different import patterns.

**Examples:**
- Using `fetch` when the project has an established API client wrapper
- Manual SQL queries when the project uses an ORM consistently
- Class components in a codebase that uses functional components exclusively

**Severity:** Medium

## Review Output Format

The Quality Reviewer produces a `### AI Pattern Findings` section in their task output. This section is present even when zero findings are identified (confirming the check was performed).

```markdown
### AI Pattern Findings

| # | Pattern | File | Location | Finding | Severity |
|---|---------|------|----------|---------|----------|
| 1 | [Pattern name] | [file path] | [line/function] | [Specific issue description] | [Critical/High/Medium/Low] |

[If no findings: "No AI-specific patterns detected. All 6 checklist items checked."]
```

## Integration

This checklist is injected into the Quality Reviewer's task in review mode spawns (`spawn-think --mode review`). The Quality Reviewer runs it as part of their code quality assessment, in addition to standard review concerns (naming, error handling, test coverage, separation of concerns).

The checklist is also available when `spawn-think --mode review` is invoked standalone.
