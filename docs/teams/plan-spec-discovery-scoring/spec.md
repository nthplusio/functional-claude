---
team: plan-spec-discovery-scoring
type: planning
mode: spec
topic: Dynamic discovery interview (#8) + formal spec quality scoring enforcement (#16)
date: 2026-02-21
status: completed
primary_artifact: spec.md
plugin_version_target: 0.18.0
---

# Implementation Spec: Dynamic Discovery + Scoring Enforcement

## Overview

Two interconnected improvements to the agent-teams plugin's pre-spawn pipeline, shipping together as v0.18.0:

1. **Dynamic discovery interview** (issue #8) — Replace the fixed 3+2 question structure with a three-layer dynamic approach (up to 12 questions), driven by ambiguity detection and feature-characteristic heuristics
2. **Formal spec quality scoring enforcement** (issue #16) — Make binary-rubric scoring a mandatory dispatch step with a required output format, closing the informal-score loophole ("78/100")

### Why Together

The two improvements are mechanically linked: dynamic questions are generated by detecting missing spec dimensions, and those same dimensions are what scoring evaluates. Shipping them separately would require scoring to run against specs that the new discovery protocol hasn't yet populated.

---

## Files to Modify

| File | Change Type |
|------|-------------|
| `shared/discovery-interview.md` | Major rewrite |
| `shared/spawn-core.md` | Add scoring to dispatch; add Scoring Invariant section |
| `shared/spec-quality-scoring.md` | Fix stale dimension count; add Enforcement Contract |
| `shared/spec-refinement.md` | Add deprecation note (keep file for evaluate-spawn/manifest references) |
| `commands/spawn-build.md` | Replace scoring step; add dedup topic labels |
| `commands/spawn-think.md` | Replace scoring step; add dedup topic labels |
| `commands/spawn-create.md` | Replace scoring step; add dedup topic labels |

---

## Part 1: shared/discovery-interview.md — Full Replacement

### Before (v0.17.0 structure)

```
## Core Questions         — 3 fixed questions, always asked
## Optional Questions     — 0–2 keyword-triggered questions
## Adaptive Skip Logic    — parse $ARGUMENTS, skip pre-answered
## Token Budget Block
## Presentation           — single AskUserQuestion batch
## Output Compilation
## Refinement Phase       — calls spec-refinement.md (separate AskUserQuestion round)
## Quality Scoring        — calls spec-quality-scoring.md
## Scenario Collection
## When to Include
## Extended Questions     — command defines extras; no cap or dedup logic
```

Problems:
- Questions triggered by keyword matching, not by measured spec gaps
- Fixed 3 questions asked even when spec is already rich
- Separate refinement phase = extra AskUserQuestion round for edge-case probing
- No deduplication between discovery and command extended questions
- "5 dimensions" typo (scoring file defines 6)

### After (v0.18.0 structure)

```
## Why This Exists
## Protocol Overview
## Step 1: Pre-Answer Detection   — parse $ARGUMENTS before any generation
## Step 2: Baseline Questions     — 3 universal, always-asked, same phrasing
## Step 3: Dynamic Question Generation
   ### Priority Tiers (P1/P2/P3)
   ### Ambiguity Detection Rules  — 6-dimension gap analysis table
   ### Feature-Characteristic Heuristics — 7 pattern-triggered questions
   ### Deduplication Contract     — spawn commands pass topic labels
## Step 4: Batching
   ### Batch Construction Rules   — 2 default + conditional 3rd
   ### Skip Semantics             — per-question and batch-level
   ### Stop Conditions
## Step 5: Mode-Specific Extended Questions
## Step 6: Spec Confirmation
## Token Budget Block              [unchanged]
## Output Compilation              [updated — no refinement reference]
## Quality Scoring                 [updated — "6 dimensions", Goal warning]
## Scenario Collection             [unchanged]
## When to Include                 [unchanged]
## Extended Questions Convention   [updated — topic labels, dedup instruction]
```

### Key Protocol Rules

**Layer 1 — Baseline (3 core questions, always-asked):**

| # | Label | Question | Spec Dimension |
|---|-------|----------|----------------|
| Q1 | Goal | "What must the system do when this is done? What can users do that they can't today?" | Goal clarity |
| Q2 | Constraints | "What are the non-negotiables — tech stack, timeline, budget, compliance?" | Constraints specificity |
| Q3 | Success | "How will you verify this works? What observable output confirms success?" | Testable success criteria |

Pre-answer skip: parse `$ARGUMENTS` first. Skip any question already clearly answered. If all 3 pre-answered, skip to dynamic generation with a single confirmation message.

**Layer 2 — Ambiguity Detection (6 dimensions):**

| Dimension | Missing Signal | Follow-up Question | Priority |
|-----------|---------------|-------------------|----------|
| Goal clarity | "better", "improve", "faster" without specifics; system-focused goal with no user-visible behavior | "When you say [vague word], what specific behavior changes? What does a user see or do differently?" | P1 if missing, P2 if partial |
| Constraints specificity | No technology/framework named, or only 1 named | "Which specific tech stack, framework, or library is this targeting? Any timeline or compliance constraints?" | P1 if missing, P2 if partial |
| Testable success | "works correctly", "feels smooth", no measurable threshold | "What's the measurable threshold? (e.g., response < 200ms, error rate < 0.1%)" | P1 if missing, P2 if partial |
| Edge case coverage | No errors, failures, invalid inputs, or limits mentioned | "What should happen when [most likely failure mode from goal]? What edge inputs or states matter?" | P1 if missing, P2 if partial |
| Acceptance criteria | No end-to-end user behavior; spec is component-focused | "Describe one end-to-end user behavior that must work — from trigger to visible outcome?" | P1 if missing, P2 if partial |
| API/library accuracy | Code or schema mentioned without version or method details | "Which specific version of [library/API]? Any method signatures or schema format?" | P2 if partial, P3 if mentioned |

Edge-case probing (formerly spec-refinement.md) is now implicit: when EdgeCases dimension is missing/partial, generate up to 2 targeted follow-ups from the Goal section. These are P1/P2. No separate refinement AskUserQuestion round.

**Layer 3 — Feature-Characteristic Heuristics (7 patterns):**

| Characteristic | Detection Pattern | Conditional Question | Dimension | Priority |
|----------------|-------------------|---------------------|-----------|----------|
| Batch operations | batch, bulk, import, export, process N records, queue, pipeline | "How should the system behave if part of the batch fails mid-run — retry, partial success, or rollback?" | Edge cases | P1 |
| Data mutation | create, update, delete, write, save, submit, post, patch | "What's the rollback or recovery behavior if the mutation fails or the user wants to undo?" | Edge cases | P1 |
| External service | API, webhook, third-party, payment, OAuth, SMS, email, external | "What happens when the external service is unavailable — fallback, queue, or fail loudly?" | Edge cases | P1 |
| Permissions/auth | role, permission, admin, owner, access control, can/cannot, restrict | "Which roles have which permissions? Any elevation, delegation, or cross-tenant scenarios?" | Constraints | P1 |
| Scheduling/time | schedule, cron, daily, recurring, at [time], reminder, deadline | "How should timezone differences be handled — user-local or server-local time?" | Constraints | P2 |
| Multi-step flow | wizard, onboarding, multi-step, checkout, flow, funnel, N-step form | "What happens if the user abandons mid-flow — is progress saved and can they resume?" | Edge cases | P2 |
| Existing similar | like [feature], similar to, extend, replace, refactor existing | "What's the existing pattern or component to follow or intentionally avoid?" | Constraints | P2 |

**Deduplication contract:** Before dynamic generation, spawn command passes its mode-specific extended question topic labels. Dynamic questions suppress any question whose topic overlaps with a provided label. Mode-specific extended questions are asked after dynamic batches, only for topics not yet covered.

**Batching rules:**

```
Batch 1 (up to 4): All P1 questions, then P2 until full
  → AskUserQuestion call
  → Re-evaluate: count dimensions still missing/partial

Batch 2 (up to 4): Remaining P1 (if any), then P2, then P3 until full
  → AskUserQuestion call if any questions remain
  → Re-evaluate: count dimensions still missing

Batch 3 (up to 4): CONDITIONAL
  → Only if ≥3 spec dimensions still missing after Batch 2
  → Contains remaining P2/P3 questions only
  → Skip if condition not met

Hard cap: 12 questions total
Practical cap: 8 (batch 3 fires only on very vague inputs)
```

**Skip semantics:**
- Per-question: blank answer or "skip this" → mark topic `user-declined`; suppress related questions in subsequent batches
- Batch-level: "skip", "enough", "proceed" → skip all remaining batches and extended questions

**Stop conditions:** (any one sufficient)
1. 12 questions reached (hard cap)
2. No P1 or P2 questions remain after re-evaluation
3. User types "skip" / "enough" / "proceed"
4. All 6 dimensions would score ✓ based on current context

---

## Part 2: shared/spawn-core.md — Changes

### Dispatch Pattern (section edit)

Add step 4, renumber current step 4→5, step 5→6:

```
1. Run prerequisites check
2. Parse --mode flag or auto-infer mode from keywords
3. Run discovery interview (from shared/discovery-interview.md + command-specific extended questions)
4. Run spec quality scoring — MANDATORY (see Scoring Invariant below)
5. Apply adaptive sizing (this file)
6. Dispatch to the appropriate legacy spawn command with compiled context
```

### New Section: Scoring Invariant

Add after Dispatch Pattern:

```markdown
## Scoring Invariant

Scoring is a mandatory gate between discovery and sizing. It MUST execute for every spawn
regardless of mode, flags, or whether the discovery interview was skipped. Sizing (step 5)
cannot begin until the score is displayed.

### Required Output Format

  Spec Quality: N/6 [Goal: ✓/✗] [Constraints: ✓/✗] [Edge Cases: ✓/✗] [Success: ✓/✗] [Acceptance: ✓/✗] [API Accuracy: ✓/✗]

This exact format is required. Non-compliant variations are explicitly prohibited:
- Point-based scores ("78/100", "85 points")
- Letter grades ("B+", "good")
- Impressionistic ratings ("mostly complete", "looks good", "nearly there")
- Omitting any dimension indicator

### Scoring Procedure

For each of the 6 dimensions in shared/spec-quality-scoring.md:
1. Read the dimension's binary question
2. Answer ✓ (yes) or ✗ (no) based ONLY on content present in the compiled Context — not inference
3. For each ✗, note the specific information that would make it pass (used in gate prompt)

### Goal Warning

If Goal Clarity fails (✗), display after the score line regardless of total score:

  ⚠ Goal Clarity failed — spec may be too vague for aligned team output. Consider clarifying before proceeding.

This is a warning only. Proceeding does not require typing "proceed".

### Gate Logic

- Default threshold: 4 passing dimensions
- Override: --min-score N flag (strip from $ARGUMENTS before passing downstream)
- N ≥ threshold: proceed to sizing
- N < threshold: display score + per-failed-dimension remediation, then prompt:

    Missing:
    - [Dimension]: [specific data needed to pass the binary question]

    Refine the spec (re-run discovery for missing areas), or type "proceed" to spawn anyway.

- User types "proceed": spawn proceeds; score included in prompt unchanged

### Spawn Prompt Inclusion

Include in the spawn prompt's Context section:

  ### Spec Quality
  Score: N/6 dimensions [Goal: ✓/✗] [Constraints: ✓/✗] [Edge Cases: ✓/✗] [Success: ✓/✗] [Acceptance: ✓/✗] [API Accuracy: ✓/✗]

6/6 confirms structural completeness — not semantic correctness.
```

---

## Part 3: shared/spec-quality-scoring.md — Changes

**Fix (line 97 of discovery-interview.md, and anywhere in this file):** "5 dimensions" → "6 dimensions"

**Add section at end:**

```markdown
## Enforcement Contract

This protocol is called from spawn-core.md as mandatory dispatch step 4. Gate logic, format
requirements, and the Goal warning all live in spawn-core.md Scoring Invariant — which is the
single source of truth for enforcement.

Spawn commands MUST NOT implement informal or alternative scoring in place of this rubric.

6/6 confirms structural completeness — not semantic correctness. A spec can pass all dimensions
and still describe the wrong feature. Scoring validates spec structure, not intent alignment.
```

---

## Part 4: shared/spec-refinement.md — Deprecation Note

Keep the file. Add this note at the top:

```markdown
> **Note (v0.18.0):** Edge-case probing from this file is now integrated into the dynamic
> discovery interview as Layer 2 ambiguity detection and Layer 3 feature-characteristic
> heuristics. The separate refinement phase (a dedicated AskUserQuestion round) has been
> removed. This file is retained as reference documentation for the edge-case question
> categories it defines, and because evaluate-spawn and the plugin manifest reference it.
```

Remove the call to this file from `discovery-interview.md` §Refinement Phase. The section is removed entirely from discovery-interview.md in v0.18.0.

---

## Part 5: Spawn Command Changes (spawn-build, spawn-think, spawn-create)

### Scoring Step Replacement (identical for all 3 commands)

Replace each command's current scoring step with:

```markdown
### Step N: Spec Quality Scoring

Evaluate the compiled Context section against the 6 binary dimensions in
`${CLAUDE_PLUGIN_ROOT}/shared/spec-quality-scoring.md`. For each dimension, answer its
binary question using ONLY content present in Context — not inference.

**Required output before proceeding to sizing:**
  Spec Quality: N/6 [Goal: ✓/✗] [Constraints: ✓/✗] [Edge Cases: ✓/✗] [Success: ✓/✗] [Acceptance: ✓/✗] [API Accuracy: ✓/✗]

Apply Goal warning and gate logic per `${CLAUDE_PLUGIN_ROOT}/shared/spawn-core.md` Scoring Invariant.
Include score in spawn prompt as `### Spec Quality`.
```

### Extended Questions — Dedup Integration

In each command's "Mode-specific extended questions" section, add before the table:

```
Pass these topic labels to the discovery interview before dynamic question generation:
[<topic labels for this mode — see table below>]
Dynamic questions will not cover these topics. Ask extended questions only for topics
not already addressed in dynamic batches.
```

**Topic labels by command and mode:**

| Command | Mode | Topic Labels |
|---------|------|-------------|
| spawn-build | feature | `[existing-context, quality-bar]` |
| spawn-build | debug | `[reproduction]` |
| spawn-think | research | `[candidate-options, depth-vs-breadth]` |
| spawn-think | planning | `[current-state, stakeholders]` |
| spawn-think | review | `[change-context, known-risks]` |
| spawn-create | design (generic) | `[target-users, design-system]` |
| spawn-create | design/component | `[target-users, design-system, component-api, composition]` |
| spawn-create | design/page-flow | `[target-users, design-system, flow-steps, data-requirements]` |
| spawn-create | design/redesign | `[target-users, design-system, pain-points, constraints-compat]` |
| spawn-create | brainstorm | `[prior-attempts, scope-boundaries]` |
| spawn-create | productivity | `[pain-points, current-tools]` |

---

## Risk Mitigations

| Risk | Severity | Mitigation in Spec |
|------|----------|--------------------|
| Over-questioning / fatigue | HIGH | Max 2 batches default; batch 3 conditional on ≥3 missing dims; batch-level skip |
| Mode-specific duplication | HIGH | Dedup contract with topic labels per mode (table above) |
| Scoring false confidence | MEDIUM | Enforcement Contract explicitly states "structural completeness, not semantic correctness"; prohibited format list names the issue #16 example |
| Regression blast radius | HIGH | 5→6 fix in both files; spec-refinement.md kept (not deleted); evaluate-spawn/manifest refs preserved |
| Batch overflow | MEDIUM | P1/P2/P3 tiers; deterministic truncation at P3 boundary; hard cap 12 |
| Goal fails without block | addressed | Warning-not-block: ⚠ warning fires, no "proceed" required |

---

## Acceptance Criteria (Dry-Run Validation)

### Discovery Interview

**Scenario A — Rich input:**
- Input: "Add PDF export to the reporting dashboard using our existing React/Node stack. Done when users can click Export and get a PDF within 3 seconds."
- Expected: All 3 baseline questions pre-answered. Ambiguity detection finds Constraints partial (stack named but no PDF library specified) → 1 P2 question asked. API Accuracy → 1 P2 question. Total: 2 questions, 1 batch. No batch 2.

**Scenario B — Vague input:**
- Input: "Build a dashboard"
- Expected: All 3 baseline questions asked (Batch 0). Ambiguity detection fires on all 6 dimensions. Batch 1: 4 P1 questions. Batch 2: remaining P1 + P2. Batch 3 may fire if ≥3 dims still missing. Total ≤ 12 questions.

**Scenario C — Pattern match:**
- Input: "Add a bulk user import feature"
- Expected: Batch ops heuristic fires → rollback question at P1. Data mutation heuristic fires → recovery question at P1. Both appear in Batch 1. Existing extended questions for the mode are NOT duplicated.

**Scenario D — User skips:**
- Input: vague, Batch 1 presented, user skips Q2 in batch
- Expected: Q2's dimension topic marked `user-declined`. Batch 2 does not re-ask Q2's topic reframed.

### Scoring Enforcement

**Scenario E — Issue #16 regression check:**
- Any spawn command runs scoring
- Expected output: exactly `Spec Quality: N/6 [Goal: ✓/✗] [Constraints: ✓/✗] [Edge Cases: ✓/✗] [Success: ✓/✗] [Acceptance: ✓/✗] [API Accuracy: ✓/✗]`
- "78/100" or any point-based score → FAIL

**Scenario F — Goal fails, total passes:**
- Spec has Goal ✗, all other dimensions ✓ → score 5/6
- Expected: 5/6 ≥ threshold (4), so gate does not fire. But Goal warning still displays: `⚠ Goal Clarity failed — spec may be too vague for aligned team output.`
- Proceeding does NOT require typing "proceed"

**Scenario G — Below threshold:**
- Score 2/6 (Goal ✗, Constraints ✗, Edge Cases ✗, Success ✗)
- Expected: Gate fires. Displays score + per-dimension remediation list. Prompts refine or "proceed".

---

## Version Target

Plugin version bump: **0.17.0 → 0.18.0**

Single release. No backward compatibility required. Both features ship together.
