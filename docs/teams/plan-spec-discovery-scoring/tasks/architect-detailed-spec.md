# Detailed Implementation Spec: discovery-interview.md + spawn-core.md

**Binding constraints source:** Task 4 user decisions + Risk Analyst constraints (all accepted)

---

## Part 1: Full Replacement — shared/discovery-interview.md

This is the complete new file content. Sections marked `[UNCHANGED]` are copied verbatim from v0.17.0.

---

```markdown
# Discovery Interview

Canonical discovery interview pattern for pre-spawn context gathering. Builds rich shared context that all teammates receive in the spawn prompt, ensuring aligned work from the first turn.

## Why This Exists

Without a discovery interview, teammates start with only a brief topic string and must independently discover constraints, goals, and context — leading to shallow, misaligned outputs. The interview moves context gathering before the spawn so teammates can start working immediately.

## Protocol Overview

The interview runs in three layers:

1. **Baseline** — 3 universal questions always asked unless pre-answered in `$ARGUMENTS`
2. **Ambiguity detection** — targeted follow-ups generated by analyzing answer quality
3. **Feature-characteristic heuristics** — conditional questions triggered by task patterns

Layers 2 and 3 are evaluated together and merged into batches. Edge-case probing (previously in spec-refinement.md) is folded into Layer 2 as Required-priority questions when the EdgeCases dimension is missing.

**Batch limits:** Max 4 questions per `AskUserQuestion` call. Default 2 batches; conditional 3rd batch only if ≥3 spec dimensions still missing after batch 2. Hard cap: 12 questions total.

## Step 1: Pre-Answer Detection

Before asking anything, parse `$ARGUMENTS` for explicit answers to the 3 baseline questions. This runs BEFORE any dynamic question generation.

Rules:
- If `$ARGUMENTS` answers 2+ baseline questions, skip those and ask only unanswered ones
- If `$ARGUMENTS` answers all 3, skip Baseline entirely — proceed to Step 2 with a confirmation: "Based on your description, here's what I understand: [summary of Goal / Constraints / Success]. Anything to adjust?"
- Mark pre-answered dimensions as `answered` — do NOT re-ask them in dynamic batches

## Step 2: Baseline Questions

Ask any unanswered baseline questions. These 3 are universal across all team types.

| # | Label | Question | Spec Dimension |
|---|-------|----------|----------------|
| Q1 | **Goal** | "What must the system do when this is done? What can users do that they can't today?" | Goal clarity |
| Q2 | **Constraints** | "What are the non-negotiables — tech stack, timeline, budget, compliance?" | Constraints specificity |
| Q3 | **Success** | "How will you verify this works? What observable output confirms success?" | Testable success criteria |

Present all unanswered baseline questions in a single `AskUserQuestion` call (max 3). Do not split baseline questions across multiple calls.

## Step 3: Dynamic Question Generation

After collecting baseline answers, run ambiguity detection and feature-characteristic scanning on ALL available context (`$ARGUMENTS` + baseline answers).

### Priority Tiers

Assign each generated question a priority before batching:

| Tier | When to assign | Batch placement |
|------|---------------|-----------------|
| **P1** | Spec dimension is `missing` (no signal at all) | Always included; fills Batch 1 first |
| **P2** | Spec dimension is `partial` (mentioned but incomplete) | Included after P1; fills remaining Batch 1 slots then Batch 2 |
| **P3** | Enrichment — dimension `answered` but elective depth useful | Only included if batch space remains after P1 + P2 |

### Ambiguity Detection Rules

Evaluate each of the 6 spec dimensions against current context. Generate questions for missing/partial dimensions:

| Dimension | Missing Signal | Ambiguity Question | Priority |
|-----------|---------------|-------------------|----------|
| Goal clarity | Goal answer contains "better", "improve", "faster", "easier", "more reliable" without specifics; or goal is system-focused with no user-visible behavior | "When you say [vague word], what specific behavior changes? What does a user see or do differently?" | P1 if missing, P2 if partial |
| Constraints specificity | No technology, framework, or constraint named anywhere; or only 1 named | "Which specific tech stack, framework, or library is this targeting? Any timeline or compliance constraints?" | P1 if missing, P2 if partial |
| Testable success criteria | Success answer uses "works correctly", "feels smooth", "users are happy", or has no measurable threshold | "What's the measurable threshold? (e.g., response < 200ms, error rate < 0.1%, task completion > 80%)" | P1 if missing, P2 if partial |
| Edge case coverage | No mention of errors, failures, invalid inputs, limits, or non-happy paths | "What should happen when [most likely failure mode from goal]? What edge inputs or states matter?" | P1 if missing, P2 if partial |
| Acceptance criteria | No end-to-end user behavior described; spec is component-focused with no full-flow scenario | "Describe one end-to-end user behavior that must work — from trigger to visible outcome?" | P1 if missing, P2 if partial |
| API/library accuracy | Code or schema mentioned without version or specific API names; or library named but method/parameter details absent | "Which specific version of [library/API]? Any method signatures or schema format you're targeting?" | P2 if partial, P3 if mentioned |

**Edge case probing (formerly spec-refinement.md):** When EdgeCases dimension is missing or partial, generate up to 2 targeted follow-ups derived from the Goal section. These are P1/P2 priority and replace the separate refinement phase. No separate "refinement phase" runs — edge case questions enter the same batch pool.

### Feature-Characteristic Heuristics

Scan all context for task patterns. Generate conditional questions when a pattern matches. Deduplicate against mode-specific extended questions before batching (see Deduplication below).

| Characteristic | Detection Pattern | Conditional Question | Dimension | Priority |
|----------------|-------------------|---------------------|-----------|----------|
| Batch operations | batch, bulk, import, export, process N records, queue, pipeline | "How should the system behave if part of the batch fails mid-run — retry, partial success, or rollback?" | Edge cases | P1 |
| Data mutation | create, update, delete, write, save, submit, post, patch | "What's the rollback or recovery behavior if the mutation fails or the user wants to undo?" | Edge cases | P1 |
| External service dependency | API, webhook, third-party, payment, OAuth, SMS, email, external | "What happens when the external service is unavailable — fallback, queue, or fail loudly?" | Edge cases | P1 |
| Permissions / auth | role, permission, admin, owner, access control, can/cannot, restrict | "Which roles have which permissions? Any elevation, delegation, or cross-tenant scenarios?" | Constraints | P1 |
| Scheduling / time | schedule, cron, daily, recurring, at [time], reminder, deadline | "How should timezone differences be handled — user-local or server-local time?" | Constraints | P2 |
| Multi-step flow | wizard, onboarding, multi-step, checkout, flow, funnel, N-step form | "What happens if the user abandons mid-flow — is progress saved and can they resume?" | Edge cases | P2 |
| Existing similar feature | like [feature], similar to, extend, replace, refactor existing | "What's the existing pattern or component to follow or intentionally avoid?" | Constraints | P2 |

### Deduplication Contract

Before generating dynamic questions, the calling spawn command MUST provide its mode-specific extended question topic labels. Dynamic question generation suppresses any question whose topic overlaps with a provided label.

Extended question topics are passed as a list when the spawn command calls the discovery interview. Example: `spawn-build --mode feature` passes `[existing-context, quality-bar]` → dynamic questions will not cover those topics.

Mode-specific extended questions are asked AFTER dynamic batches complete, only for topics not already addressed in dynamic batches.

## Step 4: Batching

Merge P1 + P2 + P3 questions into ordered batches after deduplication.

### Batch Construction Rules

```
Batch 1 (up to 4): All P1 questions, then P2 until full
  → Present via AskUserQuestion

Re-evaluate after Batch 1:
  → Count spec dimensions still missing or partial

Batch 2 (up to 4): Remaining P1 (if any), then P2, then P3 until full
  → Present if any questions remain in queue
  → Skip Batch 2 entirely if no ambiguity remains after Batch 1

Batch 3 (up to 4): Conditional
  → Present ONLY if ≥3 spec dimensions still missing after Batch 2
  → Contains remaining P2/P3 questions only (no new P1 should exist at this point)
  → Skip if condition not met

Hard cap: 12 questions total across all batches
```

### Skip Semantics

- **Per-question skip:** User leaves a question blank or writes "n/a" or "skip this". Mark that question's dimension topic as `user-declined`. Suppress related questions in subsequent batches — do not re-ask the same topic reframed.
- **Batch-level skip:** User types "skip", "enough", or "proceed". Skip all remaining batches AND mode-specific extended questions. Proceed directly to spec confirmation summary.
- **Scoring impact:** User-declined dimensions may fail their binary check in quality scoring. Scores reflect actual completeness — no inflation for skipped areas.

### Stop Conditions

Stop dynamic questioning when any of the following is true:
1. Hard cap of 12 questions reached
2. No P1 or P2 questions remain in queue after batch re-evaluation
3. User types "skip", "enough", or "proceed"
4. All 6 spec quality dimensions would score ✓ based on current context

## Step 5: Mode-Specific Extended Questions

After dynamic batches complete, the calling spawn command may ask its own extended questions for any topics not already covered. Each command defines its extended question table with topic labels.

Extended questions are presented in a single `AskUserQuestion` call (max 4). They do not count against the dynamic batch cap.

## Step 6: Spec Confirmation

After all questions complete (or are skipped), display a structured summary:

```
Based on your answers, here's the spec context:

**Goal:** [one-sentence summary]
**Constraints:** [named constraints]
**Success:** [measurable criteria]
**Edge cases:** [non-happy paths identified, or "None identified"]
**Additional context:** [other answers, if any]

Anything to adjust before I score and size the team?
```

This is a single message — not a question batch. User can correct or confirm.

## Token Budget Block [UNCHANGED]

Include this YAML structure in the spawn prompt's Context section to help teammates calibrate their effort:

```yaml
# Token Budget (informational — guides teammate effort allocation)
budget:
  discovery: 10%    # Context gathering (already done via interview)
  analysis: 30%     # Initial analysis and exploration
  feedback: 10%     # User feedback gate preparation
  execution: 40%    # Detailed work after feedback
  synthesis: 10%    # Final compilation and deliverables
```

Adjust percentages based on team type:
- **Research teams:** analysis 40%, execution 30% (research-heavy)
- **Feature teams:** analysis 20%, execution 50% (implementation-heavy)
- **Brainstorming teams:** analysis 20%, execution 40%, synthesis 20% (convergence-heavy)

## Output Compilation

Compile all interview answers into a structured `## [Team-Type] Context` section in the spawn prompt:

```markdown
## [Team-Type] Context

### Goal
[What we're trying to achieve, desired end state]

### Constraints
[Non-negotiables: budget, timeline, tech stack, etc.]

### Success Criteria
[How success is measured, what done looks like]

### Edge Cases
[Non-happy-path scenarios, failure modes, boundary conditions]

### Additional Context
[Answers to extended questions, if asked]

### Project Analysis
[Findings from codebase/document analysis, if applicable]
```

**Removed from output:** `### Additional Context` no longer references a "refinement phase" — edge case probing is now part of the dynamic question pool and its output goes directly into `### Edge Cases`.

## Quality Scoring

After the spec confirmation, run the scoring protocol from `${CLAUDE_PLUGIN_ROOT}/shared/spec-quality-scoring.md`.

Scoring evaluates **6 dimensions** of spec completeness using binary yes/no questions (not impressionistic ratings). Each dimension is pass (✓) or fail (✗).

**Display format (required):**
```
Spec Quality: N/6 [Goal: ✓/✗] [Constraints: ✓/✗] [Edge Cases: ✓/✗] [Success: ✓/✗] [Acceptance: ✓/✗] [API Accuracy: ✓/✗]
```

**Goal warning:** If Goal fails (✗), display this warning regardless of total score:
```
⚠ Goal Clarity failed — spec may be too vague for aligned team output. Consider clarifying before proceeding.
```
This is a warning, not a block. Proceeding is allowed without typing "proceed".

**Gate:** If score is below threshold (default 4), the user is prompted to refine or proceed. The score is included in the spawn prompt's Context section regardless of whether the gate fired.

**Flag:** Spawn commands accept `--min-score N` to override the default threshold.

## Scenario Collection (Feature Mode Only) [UNCHANGED]

After quality scoring (or after refinement if scoring is skipped), run the scenario collection protocol from `${CLAUDE_PLUGIN_ROOT}/shared/scenario-collection.md`.

This step collects behavioral acceptance scenarios — Given/When/Then descriptions of expected behavior — before any code is written. Scenarios serve as an external validation baseline: the user's definition of correct behavior, frozen before spawning.

**When to include:** Feature mode spawns only. Debug mode uses bug reproduction steps instead.

**Process:** Present 1 concrete example scenario derived from the Goal section, then ask the user to write at least 1 more. Minimum 2 scenarios total. Format compliance secondary to coverage.

**Output:** Scenarios written to `docs/scenarios/[feature-slug].md` and included as `### Acceptance Scenarios` in the Feature Context block.

**Skip:** User can type "skip". Skipping means no scenario file is created and quality scoring penalizes the acceptance criteria dimension.

## When to Include [UNCHANGED]

Include a discovery interview when **shared context quality drives output quality**:

| Include | Skip |
|---------|------|
| Planning teams (plan quality depends on understanding constraints) | Debug teams (bug description IS the input; urgency matters) |
| Research teams (research direction depends on knowing what matters) | Review teams (code diff IS the input; use brief interview instead) |
| Design teams (design decisions depend on understanding users) | Teams where input is already structured (spec documents, etc.) |
| Brainstorming teams (ideation quality depends on problem space) | |
| Feature teams (implementation depends on scope and criteria) | |

## Extended Questions Convention [UPDATED]

Individual spawn commands define their own extended questions beyond the dynamic pool. These are mode-specific depth questions that run AFTER dynamic batches.

When defining extended questions for spawn commands, follow this pattern:
- Assign a **topic label** (used for deduplication with dynamic questions)
- Bold the question title, then quote the actual phrasing
- Include a "When to Ask" condition to prevent over-questioning
- Max 4 extended questions per command mode

**Removed:** The previous "Core Questions" and "Optional Questions" structure. Extended questions are now the only command-level addition; the dynamic pool handles universal ambiguity detection.
```

---

## Part 2: spawn-core.md Changes

### 2a. Dispatch Pattern (Section: Dispatch Pattern)

**Current step listing:**
```
1. Run prerequisites check
2. Parse --mode flag or auto-infer mode from keywords
3. Run discovery interview (from shared/discovery-interview.md + command-specific extended questions)
4. Apply adaptive sizing (this file)
5. Dispatch to the appropriate legacy spawn command with compiled context
```

**Replacement:**
```
1. Run prerequisites check
2. Parse --mode flag or auto-infer mode from keywords
3. Run discovery interview (from shared/discovery-interview.md + command-specific extended questions)
4. **Run spec quality scoring** — MANDATORY before sizing (see Scoring Invariant below)
5. Apply adaptive sizing (this file)
6. Dispatch to the appropriate legacy spawn command with compiled context
```

### 2b. New Section — Scoring Invariant

Add after the Dispatch Pattern section:

```markdown
## Scoring Invariant

Spec quality scoring is a mandatory gate between discovery and sizing. It MUST execute for every spawn regardless of mode, flags, or whether the discovery interview was skipped. Sizing (step 5) cannot begin until the score has been displayed.

### Required Output Format

```
Spec Quality: N/6 [Goal: ✓/✗] [Constraints: ✓/✗] [Edge Cases: ✓/✗] [Success: ✓/✗] [Acceptance: ✓/✗] [API Accuracy: ✓/✗]
```

This exact format is required. Non-compliant variations (e.g., "78/100", "looks complete", "mostly done", letter grades) violate this invariant.

### Scoring Procedure

For each of the 6 dimensions in `shared/spec-quality-scoring.md`:
1. Read the dimension's binary question
2. Answer ✓ (yes) or ✗ (no) based ONLY on content present in the compiled Context — not inference
3. For each ✗, note the specific information that would make it pass (used in gate prompt)

### Goal Warning

If Goal Clarity fails (✗), display immediately after the score line:

```
⚠ Goal Clarity failed — spec may be too vague for aligned team output. Consider clarifying before proceeding.
```

This is a warning only — proceeding does not require "proceed" to be typed.

### Gate Logic

- **Default threshold:** 4 passing dimensions
- **Override:** `--min-score N` flag (strip from `$ARGUMENTS` before passing downstream)
- **Pass (N ≥ threshold):** Display score, proceed to adaptive sizing
- **Fail (N < threshold):** Display score + remediation list per failed dimension, then prompt:

  ```
  Missing:
  - [Dimension]: [specific data needed to pass the binary question]
  - [Dimension]: [specific data needed to pass the binary question]

  Refine the spec (re-run discovery for missing areas), or type "proceed" to spawn anyway.
  ```

- **User types "proceed":** Spawn proceeds. Score included in prompt unchanged.

### Spawn Prompt Inclusion

Include the score in the spawn prompt's Context section:

```markdown
### Spec Quality
Score: N/6 dimensions [Goal: ✓/✗] [Constraints: ✓/✗] [Edge Cases: ✓/✗] [Success: ✓/✗] [Acceptance: ✓/✗] [API Accuracy: ✓/✗]
```

This gives spawned teammates spec quality visibility and enables AAR correlation.

### Non-Compliant Formats

The following are explicitly prohibited:
- Point-based scores ("78/100", "85 points")
- Letter grades ("B+", "good")
- Impressionistic ratings ("mostly complete", "looks good", "nearly there")
- Omitting dimension breakdown (total only, no per-dimension indicators)

6/6 confirms structural completeness — not semantic correctness. A spec can pass all dimensions and still describe the wrong feature.
```

---

## Part 3: spec-quality-scoring.md Changes

### Fix stale dimension count

Line referencing "5 dimensions" → change to "6 dimensions" (also fix in discovery-interview.md line 97).

### Add Enforcement Contract section

```markdown
## Enforcement Contract

This protocol is called from spawn-core.md as a mandatory dispatch step (step 4). Spawn commands reference this file for scoring procedure but gate logic and format requirements live in spawn-core.md Scoring Invariant — which is the single source of truth for enforcement.

Spawn commands MUST NOT implement informal or alternative scoring in place of this rubric.
```

---

## Part 4: Spawn Command Changes

### Replacement scoring step text (identical for spawn-build, spawn-think, spawn-create)

Replace each command's current scoring step with:

```markdown
### Step N: Spec Quality Scoring

Scoring runs per `${CLAUDE_PLUGIN_ROOT}/shared/spawn-core.md` Scoring Invariant.

Evaluate each of the 6 dimensions from `${CLAUDE_PLUGIN_ROOT}/shared/spec-quality-scoring.md` individually using only its binary question. Display the required format:

  Spec Quality: N/6 [Goal: ✓/✗] [Constraints: ✓/✗] [Edge Cases: ✓/✗] [Success: ✓/✗] [Acceptance: ✓/✗] [API Accuracy: ✓/✗]

Apply Goal warning and gate logic per spawn-core.md Scoring Invariant. Include score in spawn prompt `### Spec Quality`.
```

### Extended question updates (deduplication integration)

In each spawn command's "Mode-specific extended questions" section, add one line before the table:

```
Pass these topic labels to the discovery interview before dynamic question generation:
[topic-label-1, topic-label-2, ...]
Dynamic questions will not cover these topics. Ask extended questions only for topics not addressed in dynamic batches.
```

Topic labels per command mode:

| Command | Mode | Topic Labels |
|---------|------|-------------|
| spawn-build | feature | `[existing-context, quality-bar]` |
| spawn-build | debug | `[reproduction]` |
| spawn-think | research | `[candidate-options, depth-vs-breadth]` |
| spawn-think | planning | `[current-state, stakeholders]` |
| spawn-think | review | `[change-context, known-risks]` |
| spawn-create | design | `[target-users, design-system, component-api, composition, flow-steps, data-requirements, pain-points, constraints-compat]` |
| spawn-create | brainstorm | `[prior-attempts, scope-boundaries]` |
| spawn-create | productivity | `[pain-points, current-tools]` |

---

## Part 5: spec-refinement.md Disposition

**Action:** Deprecate `shared/spec-refinement.md`.

Edge-case probing is now folded into the discovery interview's ambiguity detection layer as P1/P2 priority questions when the EdgeCases dimension is missing or partial.

The separate refinement phase is removed. No replacement file is needed.

**In discovery-interview.md:** Remove the `## Refinement Phase` section and the reference to `spec-refinement.md`. The edge-case probing behavior it described is now implicit in the ambiguity detection table (EdgeCases row).

---

## Summary of File Changes

| File | Change Type | Key Changes |
|------|-------------|-------------|
| `shared/discovery-interview.md` | Major rewrite | Replace Core/Optional Q structure with 3-layer dynamic protocol; fold refinement; add batching rules, dedup contract, priority tiers, stop conditions |
| `shared/spawn-core.md` | Addition + edit | Add scoring as dispatch step 4; add Scoring Invariant section |
| `shared/spec-quality-scoring.md` | Minor | Fix "5 dimensions" → "6 dimensions"; add Enforcement Contract |
| `shared/spec-refinement.md` | Deprecate | Remove file; behavior folded into discovery-interview.md |
| `commands/spawn-build.md` | Scoring step + dedup line | Replace loose scoring step with invariant reference; add topic labels |
| `commands/spawn-think.md` | Scoring step + dedup line | Same |
| `commands/spawn-create.md` | Scoring step + dedup line | Same |
